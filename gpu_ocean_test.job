#!/bin/bash
# Job name:
#SBATCH --job-name=gpu_ocean_test
#
# Project:
#SBATCH --account=nn9550k
#
# Wall clock limit:
#SBATCH --time=00:30:00
#
# Ask for 1 GPU (max is 2)
# Note: The environment variable CUDA_VISIBLE_DEVICES will show which GPU 
# device(s) to use. It will have values '0', '1' or '0,1' corresponding to 
# /dev/nvidia0, /dev/nvidia1 or both, respectively.
#SBATCH --partition=accel --gres=gpu:1
#
# Max memory usage per task (core) - increasing this will cost more core hours:
#SBATCH --mem-per-cpu=4G
#
# Number of tasks (for performance benchmarking, use --nodes switch):
##SBATCH --nodes=16 --ntasks-per-node=2 # gives exclusive access to all GPU-nodes
#SBATCH --nodes=4 --ntasks-per-node=1

## Set up job environment:
source /cluster/bin/jobsetup
module purge   # clear any inherited modules
set -o errexit # exit on errors

module load Python/3.5.2-foss-2016b
module load openmpi.gnu/1.10.6
module load cuda/9.1
export PATH=$HOME/.local/bin:$PATH

## Set up input and output files:
cp -r gpu_ocean/ $SCRATCH
chkfile "$SCRATCH/gpu_ocean/prototypes/scripts/mpi_run_benchmark.log"
chkfile "$SCRATCH/gpu_ocean/prototypes/scripts/netcdf*/*.nc"

cd $SCRATCH/gpu_ocean/prototypes/scripts/
mpirun python mpi_run_benchmark.py --nx 256 --ny 256 --iterations 200 --simulator CDKLM > mpi_run_benchmark.log

